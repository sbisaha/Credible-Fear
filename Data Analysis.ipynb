{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "indonesian-purse",
   "metadata": {},
   "source": [
    "# The Data\n",
    "\n",
    "Reserachers at the University of California, Berkeley's Human Rights Center organize and cleaned public immigration data from the Executive Office for Immigration Review (EOIR) and judge data from the Transcational Records Access Clearinghouse (TRAC). \n",
    "\n",
    "They then shared their cleaned data with us. \n",
    "\n",
    "The data was pulled for January 2022.  \n",
    "\n",
    "\n",
    "## Assumptions and limitations\n",
    "\n",
    "Proceedings that did not have a final decision, which made up about 41% of the initial data pull, were stripped from the data. The reserachers say \"this could be due to an oversight in recording, the data being corrupted or other reasons.\" The data does not include the asylum seekers explanation for leaving their country. Later in the notebook, we checked with another spreadsheet (one that did not have dropped cases) to see how many credible fear review cases and ones involving Cameroonians were dropped. The numbers were small and would have a negligible affect on the analysis (only one dropped case with Cameroonians and the largest percent of dropped CFR cases was in 2021 with 3.7 percent). \n",
    "\n",
    "\n",
    "## What we learned\n",
    "\n",
    "We affirmed the data in this line: A previous analysis by the students of the Human Rights Center Investigations Lab showed that immigration judges upheld more than 72% percent of all credible fear determinations they reviewed between 2018 and 2021.\n",
    "The line this pertains to in the story: \"...showing that immigration judges nationally upheld 72.23% of all credible fear determinations they reviewed between 2018 and 2021.\" \n",
    "The analysis for this can be found under the question: \"What is the national rate for credible fear reviews that side with ICE?\"\n",
    "\n",
    "We also found that Judge Landis, the judge who ruled that BJ's fears were not credible upheld 99.3 percent of the negative determinations that came before him during that time â€” keeping the Xs in place for all but 2 out of 314 asylum seekers. Between 2019 and 2021 Landis upheld all negative credible fear determinations that he reviewed.\n",
    "The analysis for this can be found under the question: \"What was judge Landis rate for siding with ICE on credible fear reviews after 2018 (only looking at 2019, 2020 and 2021)?\"\n",
    "\n",
    "The analysis for this line: \"Between 2018 and 2021, Louisiana judges upheld 93.7% of all negative credible fear determination that came before them in reviews.\"\n",
    "...can be found in the section \"What is the Louisiana rate for credible fear review that side with ICE?\"\n",
    "\n",
    "\n",
    "## Cleaning the data\n",
    "\n",
    "The NATIONALITY column, which denotes the origin country for the asylum seeker, had an error for Cameroonians where an extra character was added. One of the researchers removed the character for us. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flying-universal",
   "metadata": {},
   "source": [
    "## Importing Data \n",
    "\n",
    "'data_final.csv' is the cleaned data provided for us from the researchers. We used the Python library pandas to analyze the data. Due to the size of the file, we are only pulling in specific columns to make the processing easier. Here are the columns and what each denote, based on a codebook provided by the researchers:\n",
    "\n",
    "- NATIONALTY: Nationality of the asylum seeker.\n",
    "- DECISION_DATE-YEAR: The year on which the immigration judge rendered a decision on the proceeding\n",
    "- CHARGING_DOC_DATE_YEAR: The year the Department of Homeland Security issued the charging document to the asylum seeker\n",
    "- BASE_CITY: The code that represents the immigration court having jurisdiction over the assigned hearing location\n",
    "- DECISION: Whether the asylum seeker was granted asylum\n",
    "- JUDGE_CODE: The code that represents the immigration judge assigned to the case\n",
    "- CASE_TYPE_CFR: Credible Fear Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "radio-microwave",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BASE_CITY</th>\n",
       "      <th>JUDGE_CODE</th>\n",
       "      <th>NATIONALITY</th>\n",
       "      <th>CASE_TYPE_CFR</th>\n",
       "      <th>CHARGING_DOC_DATE_YEAR</th>\n",
       "      <th>DECISION_DATE_YEAR</th>\n",
       "      <th>DECISION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHI</td>\n",
       "      <td>ESS</td>\n",
       "      <td>HO</td>\n",
       "      <td>0</td>\n",
       "      <td>2016</td>\n",
       "      <td>2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PSD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HO</td>\n",
       "      <td>0</td>\n",
       "      <td>2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SND</td>\n",
       "      <td>LOC</td>\n",
       "      <td>VE</td>\n",
       "      <td>0</td>\n",
       "      <td>2020</td>\n",
       "      <td>2020</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SNA</td>\n",
       "      <td>YG1</td>\n",
       "      <td>EC</td>\n",
       "      <td>0</td>\n",
       "      <td>2020</td>\n",
       "      <td>2020</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SND</td>\n",
       "      <td>SS2</td>\n",
       "      <td>GT</td>\n",
       "      <td>0</td>\n",
       "      <td>2020</td>\n",
       "      <td>2020</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  BASE_CITY JUDGE_CODE NATIONALITY  CASE_TYPE_CFR  CHARGING_DOC_DATE_YEAR  \\\n",
       "0       CHI        ESS          HO              0                    2016   \n",
       "1       PSD        NaN          HO              0                    2017   \n",
       "2       SND        LOC          VE              0                    2020   \n",
       "3       SNA        YG1          EC              0                    2020   \n",
       "4       SND        SS2          GT              0                    2020   \n",
       "\n",
       "   DECISION_DATE_YEAR  DECISION  \n",
       "0                2017         0  \n",
       "1                2017         1  \n",
       "2                2020         0  \n",
       "3                2020         0  \n",
       "4                2020         0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "cols = ['NATIONALITY', 'DECISION_DATE_YEAR', 'CHARGING_DOC_DATE_YEAR', 'BASE_CITY', 'DECISION', 'JUDGE_CODE', 'CASE_TYPE_CFR']\n",
    "df = pd.read_csv('data_final.csv', usecols=cols)\n",
    "\n",
    "\n",
    "# creating a function to filter the data \n",
    "def filter_data(df, la_only, cfr_only, cm_only):\n",
    "    #if true, only look at Louisiana courts\n",
    "    if la_only == True:\n",
    "        LOU = ['NOL', 'OAK', 'JNA']\n",
    "        # filtering so we're only taking cases that happening in Louisiana\n",
    "        df = df[df[\"BASE_CITY\"].isin(LOU)]\n",
    "    #If true, only look at credible fear reviews\n",
    "    if cfr_only == True:\n",
    "        df = df[df[\"CASE_TYPE_CFR\"]==1]\n",
    "    if cm_only == True:\n",
    "        df = df[df[\"NATIONALITY\"]==\"CM\"]\n",
    "    # we only are looking at 2018, 2019, 2020 and 2021 for our analysis \n",
    "    df = df[df[\"DECISION_DATE_YEAR\"].isin([2018,2019,2020,2021])]\n",
    "    return df\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescribed-weekend",
   "metadata": {},
   "source": [
    "# Questions we asked the data (all for 2018-2021)\n",
    "\n",
    "## Credible fear review rates\n",
    "\n",
    "### What is the national rate for credible fear reviews that side with ICE?\n",
    "\n",
    "To find the answer, we filtered by credible fear review only. We also wrote a function to generate the rate. A one indicates the judge sided with the asylum seeker, while a zero indicates that the judge sided with ICE. To get the rate, we took the mean of the decision file, and subtracted that from 1. We then multiplied it by 100. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "burning-manitoba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The national rate for credible fear review that side with ICE is 72.23%.\n"
     ]
    }
   ],
   "source": [
    "def get_rate(df):\n",
    "    rate = (1-df[\"DECISION\"].mean())*100\n",
    "    return rate\n",
    "\n",
    "tdf = filter_data(df, la_only=False, cfr_only=True, cm_only=False)\n",
    "rate = get_rate(tdf)\n",
    "print(\"The national rate for credible fear review that side with ICE is {}%.\".format(rate.round(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sticky-trouble",
   "metadata": {},
   "source": [
    "### What is the Louisiana rate for credible fear review that side with ICE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "terminal-static",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Louisiana rate for credible fear review that side with ICE is 93.71%.\n"
     ]
    }
   ],
   "source": [
    "tdf = filter_data(df, la_only=True, cfr_only=True, cm_only=False)\n",
    "rate = get_rate(tdf)\n",
    "print(\"The Louisiana rate for credible fear review that side with ICE is {}%.\".format(rate.round(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stock-memorabilia",
   "metadata": {},
   "source": [
    "### What is the national rate for credible fear review siding with ICE, filtered by Cameroonians?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "serial-heart",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Cameroonian national rate for credible fear review that side with ICE is 32.41%.\n"
     ]
    }
   ],
   "source": [
    "tdf = filter_data(df, la_only=False, cfr_only=True, cm_only=True)\n",
    "rate = get_rate(tdf)\n",
    "print(\"The Cameroonian national rate for credible fear review that side with ICE is {}%.\".format(rate.round(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "light-nicholas",
   "metadata": {},
   "source": [
    "### What is the Louisiana rate for credible fear review siding with ICE, filtered by Cameroonians?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "interior-belly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Cameroonian Louisiana rate for credible fear review that side with ICE is 89.58%.\n"
     ]
    }
   ],
   "source": [
    "tdf = filter_data(df, la_only=True, cfr_only=True, cm_only=True)\n",
    "rate = get_rate(tdf)\n",
    "print(\"The Cameroonian Louisiana rate for credible fear review that side with ICE is {}%.\".format(rate.round(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passing-spotlight",
   "metadata": {},
   "source": [
    "# What were the raw numbers for Cameroonian Louisiana cases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "sorted-meeting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    43\n",
       "1     5\n",
       "Name: DECISION, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdf = filter_data(df, la_only=True, cfr_only=True, cm_only=True)\n",
    "tdf[\"DECISION\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compatible-income",
   "metadata": {},
   "source": [
    "## Number of cases\n",
    "\n",
    "### How many  credible fear review cases were there nationally between 2018 and 2021?\n",
    "For this, I filtered by credible fear reviews and did a len function to count the number of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "romantic-software",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 46475 credible fear review cases nationally.\n"
     ]
    }
   ],
   "source": [
    "cases = len(filter_data(df, la_only=False, cfr_only=True, cm_only=False))\n",
    "print(\"There are {} credible fear review cases nationally.\".format(cases))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brown-cinema",
   "metadata": {},
   "source": [
    "### How many credible fear review cases were there in Louisiana between 2018 and 2021?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "existing-soundtrack",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4152 credible fear review cases in Louisiana.\n"
     ]
    }
   ],
   "source": [
    "cases = len(filter_data(df, la_only=True, cfr_only=True, cm_only=False))\n",
    "print(\"There are {} credible fear review cases in Louisiana.\".format(cases))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supported-looking",
   "metadata": {},
   "source": [
    "### How many credible fear review cases were there nationally for Cameroonians?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "final-depression",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 324 credible fear review cases nationally involving Cameroonians.\n"
     ]
    }
   ],
   "source": [
    "cases = len(filter_data(df, la_only=False, cfr_only=True, cm_only=True))\n",
    "print(\"There are {} credible fear review cases nationally involving Cameroonians.\".format(cases))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-praise",
   "metadata": {},
   "source": [
    "### How many credible fear review cases were there in Louisiana for Cameroonians?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "irish-effort",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 48 credible fear review cases in Louisiana involving Cameroonians.\n"
     ]
    }
   ],
   "source": [
    "cases = len(filter_data(df, la_only=True, cfr_only=True, cm_only=True))\n",
    "print(\"There are {} credible fear review cases in Louisiana involving Cameroonians.\".format(cases))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-proceeding",
   "metadata": {},
   "source": [
    "## Looking at judges\n",
    "\n",
    "### How many credible fear review cases did Jude Landis have?\n",
    "\n",
    "To do this, I used the previously outlined method for getting the number of cases (checking the number of rows) after filtering the JUDGE_CODE column by \"BHL\", which is the Brent Landis' code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "swedish-prime",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jude Landis made decisions on 314 credible fear review cases.\n"
     ]
    }
   ],
   "source": [
    "tdf = filter_data(df, la_only=False, cfr_only=True, cm_only=False)\n",
    "cases = len(tdf[tdf[\"JUDGE_CODE\"]==\"BHL\"])\n",
    "print(\"Jude Landis made decisions on {} credible fear review cases.\".format(cases))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strange-empire",
   "metadata": {},
   "source": [
    "### What was Judge Landis rate for siding with ICE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "labeled-wichita",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judge Landis' rate for siding with ICE on credible fear reviews was 99.36%.\n"
     ]
    }
   ],
   "source": [
    "tdf = filter_data(df, la_only=False, cfr_only=True, cm_only=False)\n",
    "tdf = tdf[tdf[\"JUDGE_CODE\"]==\"BHL\"]\n",
    "rate = get_rate(tdf)\n",
    "print(\"Judge Landis' rate for siding with ICE on credible fear reviews was {}%.\".format(rate.round(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unauthorized-triumph",
   "metadata": {},
   "source": [
    "### How many credible fear review cases involving Cameroonians did Jude Landis have?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "speaking-scenario",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jude Landis made decisions on 11 credible fear review cases involving Cameroonians.\n"
     ]
    }
   ],
   "source": [
    "tdf = filter_data(df, la_only=False, cfr_only=True, cm_only=True)\n",
    "cases = len(tdf[tdf[\"JUDGE_CODE\"]==\"BHL\"])\n",
    "print(\"Jude Landis made decisions on {} credible fear review cases involving Cameroonians.\".format(cases))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-apple",
   "metadata": {},
   "source": [
    "### What was judge Landis rate for siding with ICE on credible fear reviews after 2018 (only looking at 2019, 2020 and 2021)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "expensive-university",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jude Landis rate for siding with ICE on credible fear reviews in 2019, 2020 and 2021 was 100.0%\n"
     ]
    }
   ],
   "source": [
    "tdf = filter_data(df, la_only=False, cfr_only=True, cm_only=False)\n",
    "tdf = tdf[tdf[\"DECISION_DATE_YEAR\"].isin([2019,2020,2021])]\n",
    "tdf = tdf[tdf[\"JUDGE_CODE\"]==\"BHL\"]\n",
    "rate = get_rate(tdf)\n",
    "print(\"Jude Landis rate for siding with ICE on credible fear reviews in 2019, 2020 and 2021 was {}%\".format(rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abandoned-placement",
   "metadata": {},
   "source": [
    "# Checking the data for dropped Cameroonians cases\n",
    "\n",
    "When cleaning the data, Berkley researchers removed all rows that did not have a final decision. To see how many cases were dropped involving Cameroonians, we compared merged.csv (provided for us by the researchers, who said this spreadsheet did not filter out cases with missing decisions) with the data_final.csv, which we did our analysis on. \n",
    "\n",
    "When comparing the years for 2018, 2019, 2020 and 2021 only one case was missing â€“ not enough to alter our analysis. \n",
    "\n",
    "### Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "armed-lewis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B_COMP_DATE</th>\n",
       "      <th>A_NAT</th>\n",
       "      <th>A_CASE_TYPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3365</th>\n",
       "      <td>2021-01-11</td>\n",
       "      <td>CM</td>\n",
       "      <td>CFR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10724</th>\n",
       "      <td>2020-05-05</td>\n",
       "      <td>CM</td>\n",
       "      <td>CFR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67815</th>\n",
       "      <td>2016-02-04</td>\n",
       "      <td>CM</td>\n",
       "      <td>CFR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194825</th>\n",
       "      <td>2016-03-07</td>\n",
       "      <td>CM</td>\n",
       "      <td>CFR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528895</th>\n",
       "      <td>2016-04-05</td>\n",
       "      <td>CM</td>\n",
       "      <td>CFR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       B_COMP_DATE A_NAT A_CASE_TYPE\n",
       "3365    2021-01-11    CM         CFR\n",
       "10724   2020-05-05    CM         CFR\n",
       "67815   2016-02-04    CM         CFR\n",
       "194825  2016-03-07    CM         CFR\n",
       "528895  2016-04-05    CM         CFR"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = [\"A_NAT\",\"A_CASE_TYPE\",\"B_COMP_DATE\"]\n",
    "merged = pd.read_csv('merged.csv', usecols=cols)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floral-pavilion",
   "metadata": {},
   "source": [
    "## First, a quick fact check: 41 percent of cases were dropped between data sets\n",
    "To confirm this, took the length of both data frames and got the percent difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "extensive-street",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-41.02452449034137"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(df)-len(merged))/len(merged)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "returning-cooper",
   "metadata": {},
   "source": [
    "## Writting the funtion to compare df_final.csv and merged.csv\n",
    "\n",
    "The function filters the merged and df_final databases so they're only credible fear reviews and Camerooneans. It also filters by the given year. The function then compares the total number of rows between each other and prints a report. Any difference between the two is noted, otherwise the report says that there is no difference.  \n",
    "\n",
    "Note that there was only one misssing credible fear review cases involving Cameroonians. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "moderate-laugh",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_spreadsheets_cm(year):\n",
    "    # filtering data_final so we're only looking at credible fear review and Cameroonians. \n",
    "    tdf = filter_data(df, la_only=False, cfr_only=True, cm_only=True)\n",
    "    # filtering merged data so we're only looking at credible fear review and Cameroonians. \n",
    "    cm = merged[merged[\"A_NAT\"]==\"CM\"]\n",
    "    cfrcm = cm[cm[\"A_CASE_TYPE\"]==\"CFR\"]\n",
    "    # filter both by the year we care about\n",
    "    universe = len(cfrcm[cfrcm[\"B_COMP_DATE\"].str.contains(str(year))])\n",
    "    cleaned = len(tdf[tdf[\"DECISION_DATE_YEAR\"]==year])\n",
    "    difference = universe-cleaned\n",
    "    print(\"Looking at {}:\".format(year))\n",
    "    print(\"There are {} cases in merged.csv.\".format(universe))\n",
    "    print(\"There are {} cases in the df_final.csv.\".format(cleaned))\n",
    "    if difference == 0:\n",
    "        print(\"Therefore, there were no dropped CM cases in {}.\".format(year))\n",
    "    else:\n",
    "        print(\"That's a difference of {} case.\".format(difference))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fundamental-honduras",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at 2018:\n",
      "There are 8 cases in merged.csv.\n",
      "There are 8 cases in the df_final.csv.\n",
      "Therefore, there were no dropped CM cases in 2018.\n"
     ]
    }
   ],
   "source": [
    "compare_spreadsheets_cm(2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "rough-whole",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at 2019:\n",
      "There are 171 cases in merged.csv.\n",
      "There are 170 cases in the df_final.csv.\n",
      "That's a difference of 1 case.\n"
     ]
    }
   ],
   "source": [
    "compare_spreadsheets_cm(2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "educated-guitar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at 2020:\n",
      "There are 138 cases in merged.csv.\n",
      "There are 138 cases in the df_final.csv.\n",
      "Therefore, there were no dropped CM cases in 2020.\n"
     ]
    }
   ],
   "source": [
    "compare_spreadsheets_cm(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "equivalent-hearing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at 2021:\n",
      "There are 8 cases in merged.csv.\n",
      "There are 8 cases in the df_final.csv.\n",
      "Therefore, there were no dropped CM cases in 2021.\n"
     ]
    }
   ],
   "source": [
    "compare_spreadsheets_cm(2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaningful-expression",
   "metadata": {},
   "source": [
    "## Checking how many credible fear review cases were missing\n",
    "\n",
    "Since our analysis was not restricted to Cameroonians, we compared the difference in credible fear review cases between the original (merged.csv) dataset and the one that had missing decision rows dropped (data_final.csv). \n",
    "\n",
    "The function below is set up in the same way as the one above â€“ the only difference is it no longer filters non-Cameroonian cases. To run the function properly, I dropped 489 rows with missing years. However, that made up a little more than half a percent of the rows and therefore would had virtually no impact on the analysis. \n",
    "\n",
    "For the years we looked at (2018, 2019, 2020, 2021) there were only a small percentage of dropped cases. 2021 had the most with 3.72% missing. That small a difference should not significantly affect our analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "recognized-peripheral",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfr_universe = merged[merged[\"A_CASE_TYPE\"]==\"CFR\"]\n",
    "# 489 rows had missing years so they were dropped. But they made up\n",
    "# a little more than half a percent of the credible fear review rows\n",
    "# so they were safe to drop without having a noteable impact on the data\n",
    "cfr_universe = cfr_universe[~cfr_universe[\"B_COMP_DATE\"].isnull()]\n",
    "def compare_spreadsheets_cfr(year):\n",
    "    tdf = filter_data(df, la_only=False, cfr_only=True, cm_only=False)\n",
    "    universe = len(cfr_universe[cfr_universe[\"B_COMP_DATE\"].str.contains(str(year))])\n",
    "    cleaned = len(tdf[tdf[\"DECISION_DATE_YEAR\"]==year])\n",
    "    difference = universe-cleaned\n",
    "    print(\"Looking at {}:\".format(year))\n",
    "    print(\"There are {} cases in the universe spreadsheet.\".format(universe))\n",
    "    print(\"There are {} cases in the cleaned spreadsheet\".format(cleaned))\n",
    "    if difference == 0:\n",
    "        print(\"Therefore, there were no dropped CM cases in {}.\".format(year))\n",
    "    else:\n",
    "        print(\"That's a difference of {} case.\".format(difference))\n",
    "        percent = ((cleaned-universe)/universe*100)\n",
    "        print(\"That's a percent difference of {}.\".format(percent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "exclusive-individual",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at 2018:\n",
      "There are 7478 cases in the universe spreadsheet.\n",
      "There are 7361 cases in the cleaned spreadsheet\n",
      "That's a difference of 117 case.\n",
      "That's a percent difference of -1.5645894624231076.\n"
     ]
    }
   ],
   "source": [
    "compare_spreadsheets_cfr(2018)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "typical-violation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at 2019:\n",
      "There are 17266 cases in the universe spreadsheet.\n",
      "There are 17145 cases in the cleaned spreadsheet\n",
      "That's a difference of 121 case.\n",
      "That's a percent difference of -0.7007992586586355.\n"
     ]
    }
   ],
   "source": [
    "compare_spreadsheets_cfr(2019)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "preliminary-strap",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at 2020:\n",
      "There are 8336 cases in the universe spreadsheet.\n",
      "There are 8189 cases in the cleaned spreadsheet\n",
      "That's a difference of 147 case.\n",
      "That's a percent difference of -1.7634357005758157.\n"
     ]
    }
   ],
   "source": [
    "compare_spreadsheets_cfr(2020)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "changing-siemens",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at 2021:\n",
      "There are 14313 cases in the universe spreadsheet.\n",
      "There are 13780 cases in the cleaned spreadsheet\n",
      "That's a difference of 533 case.\n",
      "That's a percent difference of -3.7238873751135335.\n"
     ]
    }
   ],
   "source": [
    "compare_spreadsheets_cfr(2021)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
